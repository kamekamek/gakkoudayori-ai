#!/usr/bin/env python3
"""
åˆ©ç”¨å¯èƒ½ãªGeminiãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªGeminiãƒ¢ãƒ‡ãƒ«ã¨ãã®åˆ¶é™äº‹é …ã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import os
import sys
import logging
from datetime import datetime
from typing import Dict, Any

# Google Cloud / Vertex AIé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from google.cloud import aiplatform
import vertexai
from vertexai.generative_models import GenerativeModel

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def list_available_models(project_id: str = "gakkoudayori-ai", location: str = "us-central1") -> Dict[str, Any]:
    """
    åˆ©ç”¨å¯èƒ½ãªGeminiãƒ¢ãƒ‡ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º
    
    Args:
        project_id (str): GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        location (str): ãƒªãƒ¼ã‚¸ãƒ§ãƒ³
        
    Returns:
        Dict[str, Any]: ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®è¾æ›¸
    """
    try:
        # Vertex AIåˆæœŸåŒ–
        vertexai.init(project=project_id, location=location)
        
        # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆï¼ˆ2025å¹´6æœˆæ™‚ç‚¹ï¼‰
        recommended_models = {
            "production_stable": {
                "gemini-2.5-pro-preview-06-05": {
                    "description": "Gemini 2.0 Flash - å®‰å®šç‰ˆ",
                    "input_tokens": 1048576,
                    "output_tokens": 8192,
                    "status": "stable",
                    "cost_per_1m_tokens": "$0.075 (input) / $0.30 (output)"
                },
                "gemini-1.5-flash-002": {
                    "description": "Gemini 1.5 Flash - å®‰å®šç‰ˆ",
                    "input_tokens": 1048576,
                    "output_tokens": 8192,
                    "status": "stable",
                    "cost_per_1m_tokens": "$0.075 (input) / $0.30 (output)"
                },
                "gemini-1.5-pro-002": {
                    "description": "Gemini 1.5 Pro - å®‰å®šç‰ˆ",
                    "input_tokens": 2097152,
                    "output_tokens": 8192,
                    "status": "stable",
                    "cost_per_1m_tokens": "$1.25 (input) / $5.00 (output)"
                }
            },
            "latest_available": {
                "gemini-2.5-pro-preview-06-05": {
                    "description": "Gemini 2.0 Flash - æœ€æ–°ç‰ˆ",
                    "input_tokens": 1048576,
                    "output_tokens": 8192,
                    "status": "latest",
                    "cost_per_1m_tokens": "$0.075 (input) / $0.30 (output)"
                }
            },
            "preview_experimental": {
                "gemini-2.5-flash-preview-05-20": {
                    "description": "Gemini 2.5 Flash - ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ",
                    "input_tokens": 1048576,
                    "output_tokens": 65536,
                    "status": "preview",
                    "limitations": "åˆ¶é™çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™",
                    "cost_per_1m_tokens": "å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«æ–™é‡‘"
                },
                "gemini-2.5-pro-preview-06-05": {
                    "description": "Gemini 2.5 Pro - ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ", 
                    "input_tokens": 1048576,
                    "output_tokens": 65536,
                    "status": "preview",
                    "limitations": "åˆ¶é™çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™",
                    "cost_per_1m_tokens": "å®Ÿé¨“ãƒ¢ãƒ‡ãƒ«æ–™é‡‘"
                }
            }
        }
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ
        test_results = {}
        test_prompt = "Hello, this is a connection test."
        
        for category, models in recommended_models.items():
            test_results[category] = {}
            for model_name, model_info in models.items():
                logger.info(f"Testing model: {model_name}")
                try:
                    model = GenerativeModel(model_name=model_name)
                    response = model.generate_content(test_prompt)
                    test_results[category][model_name] = {
                        "status": "âœ… Available",
                        "info": model_info,
                        "test_response_length": len(response.text) if hasattr(response, 'text') else 0
                    }
                except Exception as e:
                    test_results[category][model_name] = {
                        "status": "âŒ Unavailable",
                        "info": model_info,
                        "error": str(e)
                    }
                    
        return test_results
        
    except Exception as e:
        logger.error(f"Failed to list models: {e}")
        return {"error": str(e)}

def test_model_with_long_prompt(model_name: str, project_id: str = "gakkoudayori-ai") -> Dict[str, Any]:
    """
    ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã§é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ†ã‚¹ãƒˆã—ã¦å‡ºåŠ›åˆ¶é™ã‚’ç¢ºèª
    
    Args:
        model_name (str): ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        project_id (str): GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        
    Returns:
        Dict[str, Any]: ãƒ†ã‚¹ãƒˆçµæœ
    """
    try:
        vertexai.init(project=project_id, location="us-central1")
        model = GenerativeModel(model_name=model_name)
        
        # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‹ã‚‰JSONã¸ã®å¤‰æ›ã®ã‚ˆã†ãªé•·ã„å‡ºåŠ›ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ï¼‰
        long_prompt = """
ä»¥ä¸‹ã®éŸ³å£°è»¢å†™çµæœã‚’è©³ç´°ãªJSONå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„ï¼š

ã€Œã“ã‚“ã«ã¡ã¯ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚æ˜æ—¥ã®ä¼šè­°ã®ä»¶ã§ã™ãŒã€è³‡æ–™ã®æº–å‚™ã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿã€

ä»¥ä¸‹ã®å½¢å¼ã§JSONã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
{
  "transcript": "è»¢å†™å†…å®¹",
  "analysis": {
    "sentiment": "æ„Ÿæƒ…åˆ†æ",
    "topics": ["ãƒˆãƒ”ãƒƒã‚¯1", "ãƒˆãƒ”ãƒƒã‚¯2"],
    "intent": "æ„å›³åˆ†æ"
  },
  "structured_data": {
    "greeting": "æŒ¨æ‹¶éƒ¨åˆ†",
    "weather_comment": "å¤©æ°—ã‚³ãƒ¡ãƒ³ãƒˆ",
    "meeting_inquiry": "ä¼šè­°é–¢é€£ã®è³ªå•"
  }
}

è©³ç´°ãªåˆ†æã‚’å«ã‚ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        from vertexai.generative_models import GenerationConfig
        
        # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’èª¿æ•´ã—ã¦ãƒ†ã‚¹ãƒˆ
        generation_configs = [
            {"max_output_tokens": 1024, "temperature": 0.2},
            {"max_output_tokens": 2048, "temperature": 0.2},
            {"max_output_tokens": 4096, "temperature": 0.2},
        ]
        
        results = {}
        for i, config in enumerate(generation_configs):
            try:
                generation_config = GenerationConfig(**config)
                response = model.generate_content(long_prompt, generation_config=generation_config)
                
                results[f"test_{i+1}_tokens_{config['max_output_tokens']}"] = {
                    "status": "success",
                    "response_length": len(response.text) if hasattr(response, 'text') else 0,
                    "finish_reason": getattr(response.candidates[0], 'finish_reason', 'unknown') if response.candidates else 'no_candidates',
                    "config": config
                }
                
            except Exception as e:
                results[f"test_{i+1}_tokens_{config['max_output_tokens']}"] = {
                    "status": "error",
                    "error": str(e),
                    "config": config
                }
                
        return results
        
    except Exception as e:
        return {"error": str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 70)
    print("ğŸ” Gemini ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§èª¿æŸ»")
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().isoformat()}")
    print("=" * 70)
    
    # 1. åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
    print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
    models_info = list_available_models()
    
    if "error" in models_info:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {models_info['error']}")
        return
    
    for category, models in models_info.items():
        print(f"\nğŸ“‚ {category.upper()}:")
        for model_name, model_data in models.items():
            status = model_data.get("status", "unknown")
            info = model_data.get("info", {})
            
            print(f"  ğŸ“¦ {model_name}")
            print(f"     {status}")
            print(f"     ğŸ’¡ {info.get('description', 'No description')}")
            
            if "input_tokens" in info:
                print(f"     ğŸ“¥ Input: {info['input_tokens']:,} tokens")
            if "output_tokens" in info:
                print(f"     ğŸ“¤ Output: {info['output_tokens']:,} tokens")
            if "cost_per_1m_tokens" in info:
                print(f"     ğŸ’° Cost: {info['cost_per_1m_tokens']}")
            
            if "error" in model_data:
                print(f"     âš ï¸  Error: {model_data['error']}")
    
    # 2. æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã®ææ¡ˆ
    print("\n" + "=" * 70)
    print("ğŸ’¡ æ¨å¥¨äº‹é …:")
    print("=" * 70)
    
    print("ğŸš€ æœ¬ç•ªç’°å¢ƒç”¨ (å®‰å®šæ€§é‡è¦–):")
    print("   1. gemini-2.5-pro-preview-06-05 - æœ€æ–°ã®å®‰å®šç‰ˆã€é«˜é€Ÿ")
    print("   2. gemini-1.5-flash-002 - ä¿¡é ¼æ€§ãŒé«˜ã„å®‰å®šç‰ˆ")
    print("   3. gemini-1.5-pro-002   - é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆ")
    
    print("\nâš¡ é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç’°å¢ƒç”¨:")
    print("   1. gemini-2.5-pro-preview-06-05     - æœ€æ–°æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ")
    print("   2. gemini-1.5-flash     - ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„é¸æŠ")
    
    print("\nâŒ é¿ã‘ã‚‹ã¹ããƒ¢ãƒ‡ãƒ«:")
    print("   1. gemini-2.5-* ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆ - åˆ¶é™çš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™")
    print("   2. experimentalç‰ˆ       - äºˆå‘Šãªã—ã«å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§")
    
    # 3. ç¾åœ¨ã®ã‚¨ãƒ©ãƒ¼ã®åŸå› èª¿æŸ»
    print("\n" + "=" * 70)
    print("ğŸ” ç¾åœ¨ã®ã‚¨ãƒ©ãƒ¼èª¿æŸ»:")
    print("=" * 70)
    
    current_model = "gemini-2.5-pro-preview-06-05"
    print(f"ğŸ“ ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«: {current_model}")
    print("âš ï¸  æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
    print("   - finish_reason: MAX_TOKENS")
    print("   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³: 2,094")
    print("   - åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: 4,141")
    print("   - å‡ºåŠ›ãŒç©ºï¼ˆã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ã¾ãŸã¯MAX_TOKENSåˆ¶é™ï¼‰")
    
    print("\nğŸ› ï¸  æ¨å¥¨ä¿®æ­£:")
    print("   âœ… å®‰å®šç‰ˆãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´å®Œäº†: gemini-2.5-pro-preview-06-05")
    print("   âœ… max_output_tokensã‚’å¢—åŠ å®Œäº†: 2048 â†’ 8192")
    print("   ğŸ”§ éŸ³å£°èªè­˜ã®å•é¡Œ: ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¸ã®å¯¾å¿œè¿½åŠ æ¸ˆã¿")
    
    # 4. ä¿®æ­£ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 70)
    print("ğŸ§ª æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ:")
    print("=" * 70)
    
    recommended_model = "gemini-2.5-pro-preview-06-05"
    print(f"ğŸ”¬ ãƒ†ã‚¹ãƒˆå¯¾è±¡: {recommended_model}")
    
    test_results = test_model_with_long_prompt(recommended_model)
    if "error" not in test_results:
        for test_name, result in test_results.items():
            print(f"  ğŸ“Š {test_name}:")
            if result["status"] == "success":
                print(f"     âœ… æˆåŠŸ - ãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {result['response_length']} æ–‡å­—")
                print(f"     ğŸ çµ‚äº†ç†ç”±: {result['finish_reason']}")
            else:
                print(f"     âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
    else:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {test_results['error']}")

if __name__ == "__main__":
    main() 