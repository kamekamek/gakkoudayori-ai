""" 
Gemini APIåŸºç›¤ã‚µãƒ¼ãƒ“ã‚¹

T3-AI-002-A: Gemini APIåŸºç›¤å®Ÿè£…
- Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…
- åŸºæœ¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Ÿè£…
- APIæ¥ç¶šãƒ†ã‚¹ãƒˆé€šé

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š

1. Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã¨å–å¾—
2. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
3. APIæ¥ç¶šãƒ†ã‚¹ãƒˆæ©Ÿèƒ½
4. ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®æ¨™æº–åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä»•æ§˜æ›¸ã«å¯¾å¿œï¼š docs/30_API_endpoints.md

"""

import os
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Union, TypedDict, Literal

# Google Cloud / Vertex AIé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from google.auth import default
from google.oauth2 import service_account
from google.cloud import aiplatform
from google.api_core import exceptions as gcp_exceptions
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig

# èªè¨¼ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆ©ç”¨
from gcp_auth_service import initialize_gcp_credentials

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å‹å®šç¾©
GeminiErrorType = Literal[
    'QUOTA_EXCEEDED', 
    'PERMISSION_DENIED', 
    'MODEL_NOT_FOUND',
    'GENERAL_ERROR'
]

class GeminiResponse(TypedDict):
    """Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‹å®šç¾©"""
    text: str
    usage: Dict[str, int]
    response_time: float
    timestamp: str
    model_info: Dict[str, Any]

class GeminiError(TypedDict):
    """Gemini APIã‚¨ãƒ©ãƒ¼ã®å‹å®šç¾©"""
    error: str
    type: GeminiErrorType
    response_time: float
    timestamp: str

class ChatMessage(TypedDict):
    """Gemini APIãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‹å®šç¾©"""
    role: Literal['user', 'assistant']
    content: str

class ConnectionStatus(TypedDict):
    """Gemini APIæ¥ç¶šãƒ†ã‚¹ãƒˆçµæœã®å‹å®šç¾©"""
    success: bool
    model_info: Optional[Dict[str, Any]]
    error: Optional[str]
    response_time: float
    timestamp: str


# ==============================================================================
# Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==============================================================================

def get_gemini_client(
    project_id: str,
    credentials_path: str,
    model_name: str = "gemini-1.5-flash",
    location: str = "us-central1"
) -> Optional[GenerativeModel]:
    """
    Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã™
    
    Args:
        project_id (str): Google Cloudãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        credentials_path (str): ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_name (str, optional): ä½¿ç”¨ã™ã‚‹Geminiãƒ¢ãƒ‡ãƒ«å. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ "gemini-2.0-flash-exp"
        location (str, optional): APIãƒªãƒ¼ã‚¸ãƒ§ãƒ³. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ "us-central1"
        
    Returns:
        Optional[GenerativeModel]: åˆæœŸåŒ–ã•ã‚ŒãŸGemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¾ãŸã¯None
        
    Note:
        ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ test_credentials.json ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªãã¦ã‚‚ãƒ†ã‚¹ãƒˆç”¨ã«èªè¨¼ãŒæˆåŠŸã—ãŸã¨ã¿ãªã—ã¾ã™
    """
    start_time = time.time()
    
    try:
        # Cloud Runç’°å¢ƒã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼ã‚’ä½¿ç”¨ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        if os.getenv('K_SERVICE'):
            logger.info("Detected Cloud Run environment, using default credentials")
        elif credentials_path and os.path.exists(credentials_path):
            if not initialize_gcp_credentials(credentials_path):
                logger.error("Failed to initialize GCP credentials")
                return None
        else:
            logger.info("Using default credentials")
        
        # Vertex AIåˆæœŸåŒ–
        vertexai.init(project=project_id, location=location)
        
        # Geminiãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
        model = GenerativeModel(model_name=model_name)
        
        logger.info(f"Gemini API client initialized for model: {model_name} in {time.time() - start_time:.3f}s")
        return model
        
    except FileNotFoundError as e:
        logger.error(f"Credentials file not found: {credentials_path}, {e}")
        return None
    except gcp_exceptions.PermissionDenied as e:
        logger.error(f"Permission denied: {e}")
        return None
    except gcp_exceptions.NotFound as e:
        logger.error(f"Model not found: {model_name}, {e}")
        return None
    except Exception as e:
        logger.error(f"Failed to get Gemini API client: {e}")
        return None



# ==============================================================================
# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæ©Ÿèƒ½
# ==============================================================================

def generate_text(
    prompt: str,
    project_id: str,
    credentials_path: str,
    model_name: str = "gemini-1.5-flash",
    temperature: float = 0.2,
    max_output_tokens: int = 1024,
    top_k: int = 40,
    top_p: float = 0.8,
    location: str = "us-central1"
) -> Dict[str, Any]:
    """
    Geminiã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™
    
    Args:
        prompt (str): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        project_id (str): Google Cloudãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        credentials_path (str): ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_name (str, optional): Geminiãƒ¢ãƒ‡ãƒ«å
        temperature (float, optional): ç”Ÿæˆã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0-1)
        max_output_tokens (int, optional): æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        top_k (int, optional): ç”Ÿæˆæ™‚ã«è€ƒæ…®ã™ã‚‹ä¸Šä½kå€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³
        top_p (float, optional): ç”Ÿæˆæ™‚ã«è€ƒæ…®ã™ã‚‹top-pãƒˆãƒ¼ã‚¯ãƒ³
        location (str, optional): APIãƒªãƒ¼ã‚¸ãƒ§ãƒ³
        
    Returns:
        Dict[str, Any]: APIä»•æ§˜æ›¸ã«æº–æ‹ ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§ã®ç”Ÿæˆçµæœã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æƒ…å ±
    """
    start_time = time.time()
    timestamp = datetime.now().isoformat()
    
    try:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
        client = get_gemini_client(
            project_id=project_id,
            credentials_path=credentials_path,
            model_name=model_name,
            location=location
        )
        
        if client is None:
            return {
                "success": False,
                "error": {
                    "code": "CLIENT_INITIALIZATION_ERROR",
                    "message": "Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "details": {
                        "response_time": time.time() - start_time,
                        "timestamp": timestamp
                    }
                }
            }
        
        # ç”Ÿæˆè¨­å®šã‚’æ¸¡ã™
        generation_config = GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            top_p=top_p,
            top_k=top_k
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = client.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        generated_text = response.text
        response_time = time.time() - start_time
        
        # ä½¿ç”¨é‡æƒ…å ±ã‚’å–å¾— (åˆ©ç”¨å¯èƒ½ãªå ´åˆ)
        usage_info = {}
        if hasattr(response, "usage_metadata"):
            usage_info = {
                "prompt_token_count": getattr(response.usage_metadata, "prompt_token_count", 0),
                "candidates_token_count": getattr(response.usage_metadata, "candidates_token_count", 0),
                "total_token_count": getattr(response.usage_metadata, "total_token_count", 0)
            }
        else:
            # æ¦‚ç®—ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
            usage_info = {
                "prompt_token_count": len(prompt) // 4,  # æ¦‚ç®—å€¤
                "candidates_token_count": len(generated_text) // 4,  # æ¦‚ç®—å€¤
                "total_token_count": (len(prompt) + len(generated_text)) // 4  # æ¦‚ç®—å€¤
            }
        
        # APIä»•æ§˜æ›¸ã«åˆã‚ã›ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§è¿”å´
        result = {
            "success": True,
            "data": {
                "text": generated_text,
                "ai_metadata": {
                    "model": model_name,
                    "processing_time_ms": int(response_time * 1000),
                    "word_count": len(generated_text.split()),
                    "usage": usage_info
                },
                "timestamp": timestamp
            }
        }
        
        logger.info(f"Text generation successful. Total tokens: {usage_info['total_token_count']}, time: {response_time:.3f}s")
        return result
        
    except Exception as e:
        logger.error(f"Failed to generate text: {e}")
        return handle_gemini_error(e, start_time)


def generate_text_with_seasonal_enhancement(
    prompt: str,
    seasonal_data: Optional[Dict[str, Any]] = None,
    project_id: str = "yutori-kyoshitu",
    credentials_path: str = "secrets/gcp-credentials.json",
    model_name: str = "gemini-1.5-flash",
    temperature: float = 0.2,
    max_output_tokens: int = 1024,
    top_k: int = 40,
    top_p: float = 0.8,
    location: str = "us-central1"
) -> Dict[str, Any]:
    """
    ğŸ¨ å­£ç¯€æ„Ÿã‚’è€ƒæ…®ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆé©æ–°çš„æ–°æ©Ÿèƒ½ï¼‰
    
    Args:
        prompt (str): åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        seasonal_data (Optional[Dict[str, Any]]): å­£ç¯€æ„Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆå­£ç¯€ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆç­‰ï¼‰
        project_id (str): Google Cloudãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        credentials_path (str): ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_name (str, optional): Geminiãƒ¢ãƒ‡ãƒ«å
        temperature (float, optional): ç”Ÿæˆã®å¤šæ§˜æ€§
        max_output_tokens (int, optional): æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        top_k (int, optional): ç”Ÿæˆæ™‚ã«è€ƒæ…®ã™ã‚‹ä¸Šä½kå€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³
        top_p (float, optional): ç”Ÿæˆæ™‚ã«è€ƒæ…®ã™ã‚‹top-pãƒˆãƒ¼ã‚¯ãƒ³
        location (str, optional): APIãƒªãƒ¼ã‚¸ãƒ§ãƒ³
        
    Returns:
        Dict[str, Any]: å­£ç¯€æ„Ÿã‚’çµ±åˆã—ãŸç”Ÿæˆçµæœã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æƒ…å ±
    """
    start_time = time.time()
    
    # å­£ç¯€æ„Ÿå¼·åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
    enhanced_prompt = prompt
    
    if seasonal_data:
        season_name = seasonal_data.get('season', 'æ˜¥')
        keywords = seasonal_data.get('keywords', [])
        colors = seasonal_data.get('colors', [])
        events = seasonal_data.get('events', [])
        themes = seasonal_data.get('themes', [])
        
        seasonal_context = f"""

ğŸ¨ SEASONAL_ENHANCEMENT_CONTEXT:
ç¾åœ¨ã®å­£ç¯€: {season_name}
å­£ç¯€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords[:5])}
å­£ç¯€ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆ: {', '.join(colors[:3])}
å­¦æ ¡è¡Œäº‹: {', '.join(events)}
å­£ç¯€ãƒ†ãƒ¼ãƒ: {', '.join(themes)}

ã“ã®å­£ç¯€æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€ä»¥ä¸‹ã®è¦æ±‚ã«å¿œã˜ãŸå­¦ç´šé€šä¿¡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
- å­£ç¯€ã«é©ã—ãŸè¡¨ç¾ã‚„èªå½™ã‚’ä½¿ç”¨
- å­£ç¯€è¡Œäº‹ã‚„è‡ªç„¶ã®å¤‰åŒ–ã‚’ç¹”ã‚Šè¾¼ã‚€
- å­£ç¯€ã®è‰²å½©æ„Ÿè¦šã‚’åæ˜ ã—ãŸè¡¨ç¾
- æ•™å¸«ã‚‰ã—ã„æ¸©ã‹ã¿ã®ã‚ã‚‹æ–‡ä½“ã§
- ä¿è­·è€…ãŒå…±æ„Ÿã§ãã‚‹å­£ç¯€æ„Ÿã‚’è¡¨ç¾

ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {prompt}
"""
        enhanced_prompt = seasonal_context

    return generate_text(
        prompt=enhanced_prompt,
        project_id=project_id,
        credentials_path=credentials_path,
        model_name=model_name,
        temperature=temperature,
        max_output_tokens=max_output_tokens,
        top_k=top_k,
        top_p=top_p,
        location=location
    )

def generate_text_with_context(
    prompt: str,
    context: List[Dict[str, str]],
    project_id: str,
    credentials_path: str,
    model_name: str = "gemini-1.5-flash",
    temperature: float = 0.2,
    max_output_tokens: int = 1024,
    top_k: int = 40,
    top_p: float = 0.8,
    location: str = "us-central1"
) -> Dict[str, Any]:
    """
    Geminiã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        prompt (str): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        context (List[Dict[str, str]]): ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        project_id (str): Google Cloudãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        credentials_path (str): ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_name (str, optional): Geminiãƒ¢ãƒ‡ãƒ«å
        temperature (float, optional): ç”Ÿæˆã®å¤šæ§˜æ€§ (0-1)
        max_output_tokens (int, optional): æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        top_k (int, optional): ç”Ÿæˆæ™‚ã«è€ƒæ…®ã™ã‚‹ä¸Šä½kå€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³
        top_p (float, optional): ç”Ÿæˆæ™‚ã«è€ƒæ…®ã™ã‚‹top-pãƒˆãƒ¼ã‚¯ãƒ³
        location (str, optional): APIãƒªãƒ¼ã‚¸ãƒ§ãƒ³
        
    Returns:
        Dict[str, Any]: ç”Ÿæˆçµæœã¾ãŸã¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼æƒ…å ±
    """
    start_time = time.time()
    
    try:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
        client = get_gemini_client(
            project_id=project_id,
            credentials_path=credentials_path,
            model_name=model_name,
            location=location
        )
        
        if client is None:
            return {
                "success": False,
                "error": {
                    "code": "CLIENT_INITIALIZATION_ERROR",
                    "message": "Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "details": {
                        "response_time": time.time() - start_time,
                        "timestamp": datetime.now().isoformat()
                    }
                }
            }
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        generation_config = {
            "temperature": temperature,
            "max_output_tokens": max_output_tokens,
            "top_k": top_k,
            "top_p": top_p,
        }
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ§‹ç¯‰ã™ã‚‹ä»£ã‚ã‚Šã«ã€ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        # Vertex AIã®Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã¯ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç°¡æ˜“çš„ã«æ‰±ã†
        full_prompt = ""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        for item in context:
            if item["role"] == "user":
                full_prompt += f"User: {item['content']}\n"
            elif item["role"] == "assistant":
                full_prompt += f"Assistant: {item['content']}\n"
        
        # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        full_prompt += f"User: {prompt}\nAssistant: "
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
        response = client.generate_content(
            full_prompt,
            generation_config=generation_config
        )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        updated_context = context.copy()
        updated_context.append({"role": "user", "content": prompt})
        updated_context.append({"role": "assistant", "content": response.text})
        
        # ä½¿ç”¨é‡æƒ…å ±ã‚’å–å¾— (åˆ©ç”¨å¯èƒ½ãªå ´åˆ)
        response_time = time.time() - start_time
        timestamp = datetime.now().isoformat()
        usage_info = {}
        if hasattr(response, "usage_metadata"):
            usage_info = {
                "prompt_token_count": getattr(response.usage_metadata, "prompt_token_count", 0),
                "candidates_token_count": getattr(response.usage_metadata, "candidates_token_count", 0),
                "total_token_count": getattr(response.usage_metadata, "total_token_count", 0)
            }
        else:
            # æ¦‚ç®—ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
            usage_info = {
                "prompt_token_count": len(full_prompt) // 4,  # æ¦‚ç®—å€¤
                "candidates_token_count": len(response.text) // 4,  # æ¦‚ç®—å€¤
                "total_token_count": (len(full_prompt) + len(response.text)) // 4  # æ¦‚ç®—å€¤
            }

        # APIä»•æ§˜æ›¸ã«åˆã‚ã›ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§è¿”å´
        result = {
            "success": True,
            "data": {
                "text": response.text,
                "ai_metadata": {
                    "model": model_name,
                    "processing_time_ms": int(response_time * 1000),
                    "word_count": len(response.text.split()),
                    "usage": usage_info
                },
                "context": updated_context,
                "timestamp": timestamp
            }
        }
        
        logger.info(f"Text with context generation successful. Total tokens: {usage_info['total_token_count']}")
        return result
        
    except Exception as e:
        logger.error(f"Failed to generate text with context: {e}")
        return handle_gemini_error(e, start_time)


# ==============================================================================
# æ¥ç¶šãƒ†ã‚¹ãƒˆ
# ==============================================================================

def check_gemini_connection(
    project_id: str,
    credentials_path: str,
    model_name: str = "gemini-1.5-flash",
    location: str = "us-central1"
) -> Dict[str, Any]:
    """
    Gemini APIæ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™
    
    Args:
        project_id (str): Google Cloudãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        credentials_path (str): ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        model_name (str, optional): Geminiãƒ¢ãƒ‡ãƒ«å
        location (str, optional): APIãƒªãƒ¼ã‚¸ãƒ§ãƒ³
        
    Returns:
        Dict[str, Any]: APIä»•æ§˜æ›¸ã«æº–æ‹ ã—ãŸãƒ†ã‚¹ãƒˆçµæœ
    """
    start_time = time.time()
    timestamp = datetime.now().isoformat()
    
    try:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
        client = get_gemini_client(
            project_id=project_id,
            credentials_path=credentials_path,
            model_name=model_name,
            location=location
        )
        
        if client is None:
            return {
                "success": False,
                "error": {
                    "code": "CLIENT_INITIALIZATION_ERROR",
                    "message": "Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ",
                    "details": {
                        "processing_time_ms": int((time.time() - start_time) * 1000),
                        "timestamp": timestamp
                    }
                }
            }
        
        # ç°¡å˜ãªæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆçŸ­ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ¥ç¶šç¢ºèªï¼‰
        response = client.generate_content("Hello, test connection.")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã¨æ¥ç¶šçµæœ
        model_info = {
            "model": model_name,
            "project_id": project_id,
            "location": location,
            "version": getattr(client, "version", "unknown"),
            "capabilities": {
                "text_generation": True,
                "contextual_generation": True
            },
            "status": "available"
        }
        
        response_time = time.time() - start_time
        
        logger.info(f"Gemini API connection test successful for model {model_name} in {response_time:.3f}s")
        
        # APIä»•æ§˜æ›¸ã«æº–æ‹ ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
        return {
            "success": True,
            "data": {
                "connection_status": "ok",
                "model_info": model_info,
                "processing_time_ms": int(response_time * 1000),
                "timestamp": timestamp
            }
        }
        
    except Exception as e:
        # æ–°ã—ã„ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°é–¢æ•°ã‚’ä½¿ç”¨
        return handle_gemini_error(e, start_time)


# ==============================================================================
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
# ==============================================================================

def handle_gemini_error(error: Exception, start_time: Optional[float] = None) -> Dict[str, Any]:
    """
    Gemini APIã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†ã—APIä»•æ§˜æ›¸ã«æº–æ‹ ã—ãŸå½¢å¼ã§è¿”å´ã—ã¾ã™
    
    Args:
        error (Exception): ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼
        start_time (Optional[float], optional): å‡¦ç†é–‹å§‹æ™‚é–“
        
    Returns:
        Dict[str, Any]: APIä»•æ§˜æ›¸ã§å®šç¾©ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
    """
    error_msg = str(error)
    error_time = datetime.now().isoformat()
    processing_time_ms = None
    
    if start_time:
        processing_time_ms = int((time.time() - start_time) * 1000)
    
    # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã¨ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã®ç‰¹å®š
    if "Quota exceeded" in error_msg or "quota" in error_msg.lower():
        error_code = "QUOTA_EXCEEDED"
        error_type = "API_RATE_LIMIT"
        logger.error(f"Quota exceeded error: {error_msg}")
    elif "Permission denied" in error_msg or "permission" in error_msg.lower():
        error_code = "PERMISSION_DENIED"
        error_type = "AUTHORIZATION_ERROR"
        logger.error(f"Permission denied error: {error_msg}")
    elif "Model not found" in error_msg:
        error_code = "MODEL_NOT_FOUND"
        error_type = "RESOURCE_ERROR"
        logger.error(f"Model not found error: {error_msg}")
    elif "Invalid argument" in error_msg:
        error_code = "INVALID_ARGUMENT"
        error_type = "VALIDATION_ERROR"
        logger.error(f"Invalid argument error: {error_msg}")
    elif "Network" in error_msg or "timeout" in error_msg.lower():
        error_code = "NETWORK_ERROR"
        error_type = "CONNECTION_ERROR"
        logger.error(f"Network error: {error_msg}")
    elif "File not found" in error_msg:
        error_code = "FILE_NOT_FOUND"
        error_type = "RESOURCE_ERROR"
        logger.error(f"File not found error: {error_msg}")
    else:
        error_code = "GENERAL_ERROR"
        error_type = "INTERNAL_ERROR"
        logger.error(f"General error: {error_msg}")
    
    # APIä»•æ§˜æ›¸ã«åˆã‚ã›ãŸã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
    result = {
        "success": False,
        "error": {
            "code": error_code,
            "message": error_msg,
            "details": {
                "error_type": error_type,
                "timestamp": error_time
            }
        }
    }
    
    if processing_time_ms:
        result["error"]["details"]["processing_time_ms"] = processing_time_ms
    
    return result
